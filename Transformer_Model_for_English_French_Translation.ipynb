{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNd2mmfNit1dAlkmkWENDa1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimutaielvis/C-PROGRAMS/blob/main/Transformer_Model_for_English_French_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Setup and Installation**"
      ],
      "metadata": {
        "id": "FZ7fyiSfXbcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kimutaielvis/NEW.git\n",
        "%cd NEW\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IolT6fwgbrh",
        "outputId": "497ee3e3-2505-4aad-c419-5cd741b3447b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NEW' already exists and is not an empty directory.\n",
            "/content/NEW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install and Import Libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeHHc_q6Xy6m",
        "outputId": "39a68786-36a0-470a-db99-d3a9c1e125a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "*  Imports core libraries for numerical operations (NumPy) and deep learning (TensorFlow/Keras)\n",
        "*  Verifies TensorFlow version to ensure compatibility\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DrqTfSOrYrM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data Loading and Preprocessing python**"
      ],
      "metadata": {
        "id": "9lASbsMJY4Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and Extract Dataset\n",
        "data_path = keras.utils.get_file(\n",
        "    \"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True)\n",
        "data_path = data_path.replace(\".zip\", \"\")\n",
        "print(\"Dataset extracted to:\", data_path)"
      ],
      "metadata": {
        "id": "NkiIxDonZEmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "*   Downloads the French-English parallel corpus from TensorFlow datasets\n",
        "*   Automatically extracts the zip file\n",
        "*   Stores path to the extracted data directory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XH5yaSAYZPP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Text Processing**"
      ],
      "metadata": {
        "id": "vuWX3ZXFZfO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Process Text Pairs\n",
        "with open(data_path + \"/fra.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]  # Remove last empty line\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines[:5000]:  # Using first 5k pairs for demo\n",
        "    english, french = line.split(\"\\t\")\n",
        "    french = \"[start] \" + french + \" [end]\"  # Add special tokens\n",
        "    text_pairs.append((english, french))\n",
        "\n",
        "print(\"Sample pair:\", text_pairs[0])"
      ],
      "metadata": {
        "id": "euJhpfXHZmLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "*   Limits to 5,000 pairs for demonstration (remove limit for full dataset\n",
        "*   Adds special tokens [start] and [end] to French sentences\n",
        "*   Splits each line into English-French pairs\n",
        "*   Reads the text file line by line\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9mg165IlaQ9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Tokenization**"
      ],
      "metadata": {
        "id": "xOvWuEEpa5EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Tokenizers\n",
        "english_tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\")\n",
        "french_tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\")\n",
        "\n",
        "english_tokenizer.fit_on_texts([pair[0] for pair in text_pairs])\n",
        "french_tokenizer.fit_on_texts([pair[1] for pair in text_pairs])\n",
        "\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
        "\n",
        "Sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "print(\"English vocab size:\", english_vocab_size)\n",
        "print(\"French vocab size:\", french_vocab_size)"
      ],
      "metadata": {
        "id": "RR0GChq8Z7Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "*  Initializes tokenizers without default text filtering\n",
        "*  Builds vocabulary from the text pairs\n",
        "*  Calculates vocabulary sizes (+1 for padding token)\n",
        "*  Displays vocabulary sizes for both languages"
      ],
      "metadata": {
        "id": "2sA6oVDhaiQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Transformer Architecture**"
      ],
      "metadata": {
        "id": "mRzB7Njfaw5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Transformer model architecture\n",
        "# Define the transformer model architecture\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention and Normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs  # Residual connection\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res  # Final residual connection\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "NBJdCF95a7yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Transformer Model with Attention\n",
        "# Transformer Model with Attention\n",
        "def transformer_decoder(inputs, enc_outputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Self-Attention and Normalization\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout)(inputs, inputs)  # Self-attention\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs  # First residual connection\n",
        "\n",
        "    # Encoder-Decoder Attention\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout)(res, enc_outputs)  # Cross-attention\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + res  # Second residual connection\n",
        "\n",
        "    # Feed Forward Network\n",
        "    x = layers.Conv1D(\n",
        "        filters=ff_dim,\n",
        "        kernel_size=1,\n",
        "        activation=\"relu\")(res)  # Position-wise FFN\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(\n",
        "        filters=inputs.shape[-1],\n",
        "        kernel_size=1)(x)  # Projection back\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res  # Final output"
      ],
      "metadata": {
        "id": "AK6CoHF1co5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_outputs = layers.Dropout(0.1)(dec_outputs)\n",
        "    outputs = layers.Dense(target_vocab_size, activation=\"softmax\")(dec_outputs)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs, dec_inputs], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "[ ] # Build and train the model\n",
        "    transformer_model = build_model(english_vocab_size, french_vocab_size, sequence_length)\n",
        "    transformer_model.compile(optimizer='adam', loss='sparse_categorica_lcrossentropy', metrics=\"['accuracy'])\n",
        "\n",
        "    english_sequences = english_tokenizer.texts_to_sequences([pair[0] for pair in text_pairs])\n",
        "    french_sequences = french_tokenizer.texts_to_sequences([pair[1] for pair in text_pairs])\n",
        "\n",
        "    english_sequences = keras.preprocessing_sequence.pad_sequences(english_sequences, maxlen-sequence_length, padding=\"post\")\n",
        "    french_sequences = keras.preprocessing_sequence.pad_sequences(french_sequences, maxlen-sequence_length, padding=\"post\")\n",
        "\n",
        "    transformer_model.fit([english_sequences, french_sequences[:, i-1]], french_sequences.reshape(french_sequences.shape[0], french_sequences.shape[1], 1)[i,\n",
        "    batch_size=batch_size, epochs=10)"
      ],
      "metadata": {
        "id": "fV89MCRIeUy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}